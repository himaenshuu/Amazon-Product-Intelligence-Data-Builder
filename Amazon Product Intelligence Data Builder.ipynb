{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53246020",
   "metadata": {},
   "source": [
    "# Amazon Product Intelligence Data Builder\n",
    "\n",
    "Transform raw Amazon product responses (via RapidAPI) into an AIâ€‘ready, sentimentâ€‘aware, queryâ€‘efficient dataset stored in MongoDB.\n",
    "\n",
    "## What This Notebook Does\n",
    "- Ingests a list of ASINs from CSV and deduplicates them.\n",
    "- Fetches product metadata & reviews with controlled rate limiting and bounded retries.\n",
    "- Normalizes prices and ratings into numeric fields for analytical and range queries.\n",
    "- Infers product type (phone, laptop, tablet, audio, watch) from title heuristics.\n",
    "- Samples and categorizes reviews (positive / negative / neutral) for sentimentâ€‘driven chat use cases.\n",
    "- Persists enriched records to MongoDB with performanceâ€‘oriented indexes (price, rating, brand, type, text search).\n",
    "- Provides example MongoDB queries for ChatAI integration.\n",
    "\n",
    "## Why Itâ€™s Useful\n",
    "Optimized for powering conversational recommendation, filtering (e.g. â€œphones under 20k, rating > 4â€), and sentiment summaries without heavy postâ€‘processing. The schema aligns with natural language intent translation.\n",
    "\n",
    "## High-Level Pipeline\n",
    "1. Load configuration & environment variables.\n",
    "2. Read ASINs from `asins.csv`.\n",
    "3. Fetch product details (retry + rate limit).\n",
    "4. Normalize & enrich (pricing, rating, sentiment, type detection).\n",
    "5. Save to JSON and optionally clean errors externally.\n",
    "6. Upsert into MongoDB and build indexes.\n",
    "7. Run sample AIâ€‘oriented queries.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- RapidAPI key in `.env` (`RAPID_API_KEY`).\n",
    "- Local or Atlas MongoDB instance.\n",
    "- CSV file containing an `asin` column.\n",
    "\n",
    "## Execution Order (Cells)\n",
    "1. Install dependencies.\n",
    "2. Imports & environment load.\n",
    "3. ASIN reader.\n",
    "4. API fetcher.\n",
    "5. Normalization logic.\n",
    "6. Save helpers.\n",
    "7. MongoDB push & indexing.\n",
    "8. Scrape runner.\n",
    "9. Push cleaned JSON.\n",
    "10. Sample queries for ChatAI.\n",
    "\n",
    "Proceed to the next cell to set up dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cb09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q requests retrying pandas pymongo python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed11c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import requests \n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from retrying import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090df1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Load from environment if available, else use defaults\n",
    "RAPIDAPI_KEY = os.getenv('RAPID_API_KEY') \n",
    "RAPIDAPI_HOST = os.getenv('RAPIDAPI_HOST', 'real-time-amazon-data.p.rapidapi.com')\n",
    "MONGO_URI = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/') # if not then else\n",
    "MONGO_DB = os.getenv('MONGO_DB', 'amazon_scrape_db')\n",
    "MONGO_COLLECTION = os.getenv('MONGO_COLLECTION', 'amazon_scrape')\n",
    "\n",
    "API_URL = 'https://real-time-amazon-data.p.rapidapi.com/product-details'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0e3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Read ASINs from a CSV file\n",
    "\n",
    "def read_asins_from_csv(path: str) -> List[str]:\n",
    "   df = pd.read_csv(path, dtype=str)\n",
    "   if 'asin' not in df.columns:\n",
    "      raise ValueError('CSV must contain a column named \"asin\".')\n",
    "   asins = df['asin'].dropna().str.strip().str.upper().unique().tolist()\n",
    "   return asins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30eda085",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Fetch data from RapidAPI\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@retry(stop_max_attempt_number=2, wait_exponential_multiplier=1000, wait_exponential_max=10000)\n",
    "def call_api_for_asin(asin: str, country: str = 'IN') -> Dict:\n",
    "\theaders = {\n",
    "\t\t'X-RapidAPI-Key': RAPIDAPI_KEY,\n",
    "\t\t'X-RapidAPI-Host': RAPIDAPI_HOST\n",
    "\t}\n",
    "\tparams = {\n",
    "\t\t'asin': asin,\n",
    "\t\t'country': 'IN'  # Add country parameter - 'IN' for India, 'US' for USA\n",
    "\t}\n",
    "\t\n",
    "\tresponse = requests.get(API_URL, headers=headers, params=params, timeout=15)\n",
    "\tif response.status_code == 200:\n",
    "\t\treturn response.json()\n",
    "\telse:\n",
    "\t\tlogger.warning(f\"Error {response.status_code} for ASIN {asin}\")\n",
    "\t\tresponse.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70250460",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Normalize API Response\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "def normalize_api_response(asin: str, data: Dict) -> Dict:\n",
    "\t# Extract the nested 'data' field from API response\n",
    "\tproduct_data = data.get('data', {}) if isinstance(data, dict) else {}\n",
    "\t\n",
    "\t# Extract all reviews (top_reviews + top_reviews_global)\n",
    "\tall_reviews = []\n",
    "\t\n",
    "\t# Get top reviews\n",
    "\tif product_data.get('top_reviews'):\n",
    "\t\tall_reviews.extend(product_data.get('top_reviews', []))\n",
    "\t\n",
    "\t# Get top reviews global\n",
    "\tif product_data.get('top_reviews_global'):\n",
    "\t\tall_reviews.extend(product_data.get('top_reviews_global', []))\n",
    "\t\n",
    "\t# Remove duplicates based on review_title and review_comment\n",
    "\tseen = set()\n",
    "\tunique_reviews = []\n",
    "\tfor review in all_reviews:\n",
    "\t\treview_key = (review.get('review_title'), review.get('review_comment')[:100] if review.get('review_comment') else None)\n",
    "\t\tif review_key not in seen:\n",
    "\t\t\tseen.add(review_key)\n",
    "\t\t\tunique_reviews.append(review)\n",
    "\t\n",
    "\t# Select 10 random reviews (or fewer if less than 10 available)\n",
    "\tselected_reviews = random.sample(unique_reviews, min(10, len(unique_reviews))) if unique_reviews else []\n",
    "\t\n",
    "\t# Parse price to numeric value for easy querying\n",
    "\tdef parse_price(price_str):\n",
    "\t\t\"\"\"Extract numeric price from string like 'â‚¹19,999' or '$299.99'\"\"\"\n",
    "\t\tif not price_str:\n",
    "\t\t\treturn None\n",
    "\t\t# Remove currency symbols and commas, extract number\n",
    "\t\tnumeric = re.sub(r'[^\\d.]', '', str(price_str))\n",
    "\t\ttry:\n",
    "\t\t\treturn float(numeric) if numeric else None\n",
    "\t\texcept:\n",
    "\t\t\treturn None\n",
    "\t\n",
    "\tprice_numeric = parse_price(product_data.get('product_price'))\n",
    "\toriginal_price_numeric = parse_price(product_data.get('product_original_price'))\n",
    "\t\n",
    "\t# Parse rating to float\n",
    "\tdef parse_rating(rating_str):\n",
    "\t\t\"\"\"Convert rating string to float\"\"\"\n",
    "\t\tif not rating_str:\n",
    "\t\t\treturn None\n",
    "\t\ttry:\n",
    "\t\t\treturn float(str(rating_str).replace(' out of 5 stars', '').strip())\n",
    "\t\texcept:\n",
    "\t\t\treturn None\n",
    "\t\n",
    "\trating_numeric = parse_rating(product_data.get('product_star_rating'))\n",
    "\t\n",
    "\t# Categorize reviews as positive, negative, neutral\n",
    "\tpositive_reviews = []\n",
    "\tnegative_reviews = []\n",
    "\tneutral_reviews = []\n",
    "\t\n",
    "\tfor review in selected_reviews:\n",
    "\t\treview_data = {\n",
    "\t\t\t'rating': review.get('review_star_rating'),\n",
    "\t\t\t'title': review.get('review_title'),\n",
    "\t\t\t'comment': review.get('review_comment'),\n",
    "\t\t\t'verified': review.get('review_verified_purchase'),\n",
    "\t\t\t'helpful_count': review.get('review_helpful_count')\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# Parse review rating\n",
    "\t\treview_rating = parse_rating(review.get('review_star_rating'))\n",
    "\t\t\n",
    "\t\tif review_rating and review_rating >= 4:\n",
    "\t\t\tpositive_reviews.append(review_data)\n",
    "\t\telif review_rating and review_rating <= 2:\n",
    "\t\t\tnegative_reviews.append(review_data)\n",
    "\t\telse:\n",
    "\t\t\tneutral_reviews.append(review_data)\n",
    "\t\n",
    "\t# Extract product title and create searchable keywords\n",
    "\ttitle = product_data.get('product_title', '')\n",
    "\ttitle_lower = title.lower() if title else ''\n",
    "\t\n",
    "\t# Detect product type from title\n",
    "\tproduct_type = None\n",
    "\tif any(keyword in title_lower for keyword in ['phone', 'mobile', 'smartphone', 'iphone', 'samsung', 'oneplus', 'redmi', 'realme']):\n",
    "\t\tproduct_type = 'phone'\n",
    "\telif any(keyword in title_lower for keyword in ['laptop', 'notebook', 'macbook', 'chromebook']):\n",
    "\t\tproduct_type = 'laptop'\n",
    "\telif any(keyword in title_lower for keyword in ['headphone', 'earphone', 'earbud', 'airpod']):\n",
    "\t\tproduct_type = 'audio'\n",
    "\telif any(keyword in title_lower for keyword in ['watch', 'smartwatch']):\n",
    "\t\tproduct_type = 'watch'\n",
    "\telif any(keyword in title_lower for keyword in ['tablet', 'ipad']):\n",
    "\t\tproduct_type = 'tablet'\n",
    "\t\n",
    "\t# Extract only essential fields - optimized for AI chat queries\n",
    "\treturn {\n",
    "\t\t'asin': asin,\n",
    "\t\t'title': title,\n",
    "\t\t'brand': product_data.get('product_information', {}).get('Brand'),\n",
    "\t\t'product_type': product_type,  # For easy filtering by category\n",
    "\t\t\n",
    "\t\t# Price fields - numeric for range queries\n",
    "\t\t'price': product_data.get('product_price'),  # Original string\n",
    "\t\t'price_numeric': price_numeric,  # For queries like \"under 20k\"\n",
    "\t\t'original_price': product_data.get('product_original_price'),\n",
    "\t\t'original_price_numeric': original_price_numeric,\n",
    "\t\t'currency': product_data.get('currency'),\n",
    "\t\t'country': product_data.get('country'),\n",
    "\t\t\n",
    "\t\t# Rating fields - numeric for comparisons\n",
    "\t\t'rating': product_data.get('product_star_rating'),  # Original string\n",
    "\t\t'rating_numeric': rating_numeric,  # For queries like \"rating > 4\"\n",
    "\t\t'reviews_count': product_data.get('product_num_ratings'),\n",
    "\t\t\n",
    "\t\t# Product details\n",
    "\t\t'availability': product_data.get('product_availability'),\n",
    "\t\t'product_url': product_data.get('product_url'),\n",
    "\t\t'product_photo': product_data.get('product_photo'),\n",
    "\t\t'is_best_seller': product_data.get('is_best_seller'),\n",
    "\t\t'sales_volume': product_data.get('sales_volume'),\n",
    "\t\t'category': product_data.get('category', {}).get('name'),\n",
    "\t\t'product_description': product_data.get('product_description'),\n",
    "\t\t'customers_say': product_data.get('customers_say'),\n",
    "\t\t\n",
    "\t\t# Reviews categorized for AI queries\n",
    "\t\t'positive_reviews': positive_reviews,  # Rating >= 4\n",
    "\t\t'negative_reviews': negative_reviews,  # Rating <= 2\n",
    "\t\t'neutral_reviews': neutral_reviews,    # Rating 3\n",
    "\t\t\n",
    "\t\t# Stats for quick reference\n",
    "\t\t'total_reviews_available': len(unique_reviews),\n",
    "\t\t'positive_reviews_count': len(positive_reviews),\n",
    "\t\t'negative_reviews_count': len(negative_reviews),\n",
    "\t\t'neutral_reviews_count': len(neutral_reviews),\n",
    "\t\t\n",
    "\t\t# Metadata\n",
    "\t\t'scraped_at': None,  # Will be set during MongoDB insert\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 7: Save results to JSON\n",
    "\n",
    "import json\n",
    "\n",
    "def save_to_json(records: List[Dict], output_path: str):\n",
    "    \"\"\"Save scraped product data into a JSON file.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=4)\n",
    "    logger.info(f\"Saved {len(records)} records to {output_path}\")\n",
    "\n",
    "def save_full_backup(records: List[Dict], backup_path: str = 'amazon_results_full_backup.json'):\n",
    "    \"\"\"\n",
    "    Optional: Save complete raw data as backup (includes all API response data).\n",
    "    Only use this if you need to preserve the full API response for later analysis.\n",
    "    \"\"\"\n",
    "    with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(f\" Full backup saved to {backup_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23300ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 8: Push data to MongoDB with optimized indexes for AI chat queries\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def push_to_mongo(records: List[Dict]):\n",
    "\tclient = MongoClient(MONGO_URI)\n",
    "\tdb = client[MONGO_DB]\n",
    "\tcollection = db[MONGO_COLLECTION]\n",
    "\n",
    "\t# Add timestamp to each record\n",
    "\tfor r in records:\n",
    "\t\tr['scraped_at'] = datetime.utcnow()\n",
    "\n",
    "\toperations = []\n",
    "\tfor r in records:\n",
    "\t\toperations.append(UpdateOne({'asin': r['asin']}, {'$set': r}, upsert=True))\n",
    "\n",
    "\tif operations:\n",
    "\t\tcollection.bulk_write(operations)\n",
    "\t\tlogger.info(f\"Inserted/Updated {len(operations)} records into MongoDB.\")\n",
    "\t\n",
    "\t# Create indexes for fast AI query performance\n",
    "\ttry:\n",
    "\t\t# Index for price range queries: \"show me phones under 20k\"\n",
    "\t\tcollection.create_index([('price_numeric', 1)])\n",
    "\t\tcollection.create_index([('product_type', 1), ('price_numeric', 1)])\n",
    "\t\t\n",
    "\t\t# Index for rating queries: \"show highly rated products\"\n",
    "\t\tcollection.create_index([('rating_numeric', -1)])\n",
    "\t\t\n",
    "\t\t# Index for brand queries: \"show me Samsung phones\"\n",
    "\t\tcollection.create_index([('brand', 1)])\n",
    "\t\t\n",
    "\t\t# Index for product type filtering\n",
    "\t\tcollection.create_index([('product_type', 1)])\n",
    "\t\t\n",
    "\t\t# Text index for natural language search on title and description\n",
    "\t\tcollection.create_index([\n",
    "\t\t\t('title', 'text'),\n",
    "\t\t\t('product_description', 'text'),\n",
    "\t\t\t('brand', 'text')\n",
    "\t\t], name='text_search_index')\n",
    "\t\t\n",
    "\t\t# Compound index for complex queries: \"phones under 20k with rating > 4\"\n",
    "\t\tcollection.create_index([\n",
    "\t\t\t('product_type', 1),\n",
    "\t\t\t('price_numeric', 1),\n",
    "\t\t\t('rating_numeric', -1)\n",
    "\t\t])\n",
    "\t\t\n",
    "\t\tlogger.info(\"Created indexes for optimized AI chat queries\")\n",
    "\texcept Exception as e:\n",
    "\t\tlogger.warning(f\"Some indexes may already exist: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d75f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 9A: Scrape Amazon Data and Save to JSON\n",
    "# Run this cell first â€” it will read ASINs from a CSV, fetch product data using the API, and save all results into a JSON file.\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "\n",
    "# --- Rate Limiter ---\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls: int, period: float):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.lock = threading.Lock()\n",
    "        self.calls = []\n",
    "\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = datetime.now()\n",
    "            self.calls = [t for t in self.calls if now - t < timedelta(seconds=self.period)]\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = (self.calls[0] + timedelta(seconds=self.period) - now).total_seconds()\n",
    "                logger.info(f\"Rate limit reached. Sleeping for {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            self.calls.append(datetime.now())\n",
    "\n",
    "rate_limiter = RateLimiter(max_calls=2, period=1.0)  # 2 requests per second to avoid 429 errors\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_ASINS_TO_SCRAPE = 300  # Set how many ASINs to scrape (None = scrape all)\n",
    "input_csv = 'asins.csv'\n",
    "output_json = 'amazon_results.json'\n",
    "\n",
    "# --- Scraper Logic ---\n",
    "asins = read_asins_from_csv(input_csv)\n",
    "\n",
    "# Limit the number of ASINs if MAX_ASINS_TO_SCRAPE is set\n",
    "if MAX_ASINS_TO_SCRAPE is not None:\n",
    "    asins = asins[:MAX_ASINS_TO_SCRAPE]\n",
    "    logger.info(f\"Limiting scrape to first {len(asins)} ASINs\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for asin in tqdm(asins, desc='Scraping ASINs'):\n",
    "    rate_limiter.wait()\n",
    "    try:\n",
    "        data = call_api_for_asin(asin)\n",
    "        record = normalize_api_response(asin, data)\n",
    "        results.append(record)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch ASIN {asin}: {e}\")\n",
    "        results.append({'asin': asin, 'error': str(e)})\n",
    "\n",
    "save_to_json(results, output_json)\n",
    "logger.info(f\"Scraping completed. Results saved to {output_json}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 16:21:37,163 - INFO - Inserted/Updated 98 records into MongoDB.\n",
      "2025-11-13 16:21:37,433 - INFO - âœ… Created indexes for optimized AI chat queries\n",
      "2025-11-13 16:21:37,440 - INFO - âœ… Successfully pushed 98 records from 'amazon_results_cleaned.json' to MongoDB.\n",
      "2025-11-13 16:21:37,433 - INFO - âœ… Created indexes for optimized AI chat queries\n",
      "2025-11-13 16:21:37,440 - INFO - âœ… Successfully pushed 98 records from 'amazon_results_cleaned.json' to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "# Step 9B: Push Scraped JSON Data to MongoDB\n",
    "# Run this cell after scraping â€” it loads the JSON file and uploads all records to MongoDB.\n",
    "\n",
    "json_path = 'amazon_results_cleaned.json'\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    logger.error(f\"JSON file '{json_path}' not found. Please run the previous cell first.\")\n",
    "else:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    if not records:\n",
    "        logger.warning(\"No records found in JSON file to push to MongoDB.\")\n",
    "    else:\n",
    "        push_to_mongo(records)\n",
    "        logger.info(f\"Successfully pushed {len(records)} records from '{json_path}' to MongoDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1346424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Data Summary:\n",
      "   Total Records: 300\n",
      "   File Size: 0.57 MB\n",
      "   Avg per record: 1.94 KB\n",
      "\n",
      "ðŸ“¦ Fields in each record:\n",
      "   asin, title, brand, product_type, price, price_numeric, original_price, original_price_numeric, currency, country, rating, rating_numeric, reviews_count, availability, product_url, product_photo, is_best_seller, sales_volume, category, product_description, customers_say, positive_reviews, negative_reviews, neutral_reviews, total_reviews_available, positive_reviews_count, negative_reviews_count, neutral_reviews_count, scraped_at\n"
     ]
    }
   ],
   "source": [
    "# Check JSON file size and record count\n",
    "import os\n",
    "\n",
    "json_file = 'amazon_results.json'\n",
    "if os.path.exists(json_file):\n",
    "    file_size_mb = os.path.getsize(json_file) / (1024 * 1024)\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"ðŸ“Š Data Summary:\")\n",
    "    print(f\"   Total Records: {len(data)}\")\n",
    "    print(f\"   File Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"   Avg per record: {file_size_mb / len(data) * 1024:.2f} KB\")\n",
    "    \n",
    "    # Sample record to show what's included\n",
    "    if data:\n",
    "        print(f\"\\nFields in each record:\")\n",
    "        print(f\"   {', '.join(data[0].keys())}\")\n",
    "else:\n",
    "    print(f\"File '{json_file}' not found. Run the scraping cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34427ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries for ChatAI integration\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[MONGO_DB]\n",
    "collection = db[MONGO_COLLECTION]\n",
    "\n",
    "# Query 1: \"Show me all phones under 20k\"\n",
    "print(\"ðŸ“± Query 1: All phones under â‚¹20,000\")\n",
    "phones_under_20k = list(collection.find(\n",
    "    {\n",
    "        'product_type': 'phone',\n",
    "        'price_numeric': {'$lte': 20000}\n",
    "    },\n",
    "    {\n",
    "        'title': 1, \n",
    "        'brand': 1, \n",
    "        'price': 1, \n",
    "        'rating_numeric': 1,\n",
    "        '_id': 0\n",
    "    }\n",
    ").sort('price_numeric', 1))\n",
    "print(f\"Found {len(phones_under_20k)} phones\")\n",
    "for phone in phones_under_20k[:3]:  # Show first 3\n",
    "    print(f\"  â€¢ {phone.get('title', 'N/A')[:60]} - {phone.get('price', 'N/A')} (Rating: {phone.get('rating_numeric', 'N/A')})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Query 2: \"What are positive reviews of iPhone 14\"\n",
    "print(\"â­ Query 2: Positive reviews of iPhone 14\")\n",
    "iphone_reviews = collection.find_one(\n",
    "    {\n",
    "        '$text': {'$search': 'iPhone 14'},\n",
    "        'positive_reviews_count': {'$gt': 0}\n",
    "    },\n",
    "    {\n",
    "        'title': 1,\n",
    "        'positive_reviews': 1,\n",
    "        'positive_reviews_count': 1,\n",
    "        '_id': 0\n",
    "    }\n",
    ")\n",
    "\n",
    "if iphone_reviews:\n",
    "    print(f\"Product: {iphone_reviews.get('title', 'N/A')[:80]}\")\n",
    "    print(f\"Total positive reviews: {iphone_reviews.get('positive_reviews_count', 0)}\")\n",
    "    print(\"\\nPositive Reviews:\")\n",
    "    for i, review in enumerate(iphone_reviews.get('positive_reviews', [])[:3], 1):\n",
    "        print(f\"\\n  Review {i}:\")\n",
    "        print(f\"    Rating: {review.get('rating', 'N/A')}\")\n",
    "        print(f\"    Title: {review.get('title', 'N/A')}\")\n",
    "        print(f\"    Comment: {review.get('comment', 'N/A')[:150]}...\")\n",
    "else:\n",
    "    print(\"No iPhone 14 found in database\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Query 3: \"Show highly rated phones under 25k\"\n",
    "print(\"ðŸ† Query 3: Highly rated phones (rating >= 4) under â‚¹25,000\")\n",
    "top_phones = list(collection.find(\n",
    "    {\n",
    "        'product_type': 'phone',\n",
    "        'price_numeric': {'$lte': 25000},\n",
    "        'rating_numeric': {'$gte': 4.0}\n",
    "    },\n",
    "    {\n",
    "        'title': 1,\n",
    "        'brand': 1,\n",
    "        'price': 1,\n",
    "        'rating_numeric': 1,\n",
    "        'positive_reviews_count': 1,\n",
    "        '_id': 0\n",
    "    }\n",
    ").sort('rating_numeric', -1).limit(5))\n",
    "\n",
    "print(f\"Found {len(top_phones)} phones\")\n",
    "for phone in top_phones:\n",
    "    print(f\"  â€¢ {phone.get('title', 'N/A')[:60]}\")\n",
    "    print(f\"    Price: {phone.get('price', 'N/A')} | Rating: {phone.get('rating_numeric', 'N/A')} | Positive Reviews: {phone.get('positive_reviews_count', 0)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Query 4: \"What are negative reviews of [product]\"\n",
    "print(\"ðŸ‘Ž Query 4: Negative reviews (example query)\")\n",
    "negative_example = collection.find_one(\n",
    "    {\n",
    "        'negative_reviews_count': {'$gt': 0}\n",
    "    },\n",
    "    {\n",
    "        'title': 1,\n",
    "        'negative_reviews': 1,\n",
    "        'negative_reviews_count': 1,\n",
    "        '_id': 0\n",
    "    }\n",
    ")\n",
    "\n",
    "if negative_example:\n",
    "    print(f\"Product: {negative_example.get('title', 'N/A')[:80]}\")\n",
    "    print(f\"Total negative reviews: {negative_example.get('negative_reviews_count', 0)}\")\n",
    "    for i, review in enumerate(negative_example.get('negative_reviews', [])[:2], 1):\n",
    "        print(f\"\\n  Negative Review {i}:\")\n",
    "        print(f\"    Rating: {review.get('rating', 'N/A')}\")\n",
    "        print(f\"    Title: {review.get('title', 'N/A')}\")\n",
    "        print(f\"    Comment: {review.get('comment', 'N/A')[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Query 5: \"Best seller phones\"\n",
    "print(\"ðŸ’Ž Query 5: Best seller phones\")\n",
    "best_sellers = list(collection.find(\n",
    "    {\n",
    "        'product_type': 'phone',\n",
    "        'is_best_seller': True\n",
    "    },\n",
    "    {\n",
    "        'title': 1,\n",
    "        'brand': 1,\n",
    "        'price': 1,\n",
    "        'rating_numeric': 1,\n",
    "        '_id': 0\n",
    "    }\n",
    ").limit(5))\n",
    "\n",
    "print(f\"Found {len(best_sellers)} best seller phones\")\n",
    "for phone in best_sellers:\n",
    "    print(f\"  â€¢ {phone.get('title', 'N/A')[:60]} - {phone.get('price', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38832b",
   "metadata": {},
   "source": [
    "## ChatAI Integration Guide\n",
    "\n",
    "### Key Features Added for ChatAI:\n",
    "\n",
    "1. **Numeric Fields**: `price_numeric`, `rating_numeric` for range queries\n",
    "2. **Product Type Detection**: Auto-categorizes products (phone, laptop, audio, etc.)\n",
    "3. **Categorized Reviews**: Separate positive/negative/neutral review arrays\n",
    "4. **Optimized Indexes**: Fast queries for price, rating, brand, product type\n",
    "5. **Text Search**: Natural language search on title, description, brand\n",
    "\n",
    "### Sample ChatAI Query Mappings:\n",
    "\n",
    "| User Query | MongoDB Query |\n",
    "|-----------|---------------|\n",
    "| \"Show me phones under 20k\" | `{product_type: 'phone', price_numeric: {$lte: 20000}}` |\n",
    "| \"Positive reviews of iPhone 14\" | `{$text: {$search: 'iPhone 14'}, positive_reviews_count: {$gt: 0}}` |\n",
    "| \"Highly rated laptops\" | `{product_type: 'laptop', rating_numeric: {$gte: 4.5}}` |\n",
    "| \"Samsung phones between 15k-30k\" | `{brand: 'Samsung', product_type: 'phone', price_numeric: {$gte: 15000, $lte: 30000}}` |\n",
    "| \"Best seller products\" | `{is_best_seller: true}` |\n",
    "\n",
    "### To integrate with your ChatAI:\n",
    "1. Parse user intent to identify: product type, price range, brand, rating requirement\n",
    "2. Use text search for product names (iPhone, Galaxy, etc.)\n",
    "3. Filter by `positive_reviews` or `negative_reviews` arrays\n",
    "4. Return results with title, price, rating, and relevant reviews\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
